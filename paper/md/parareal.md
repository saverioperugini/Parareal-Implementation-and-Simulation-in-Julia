PARAREAL ALGORITHM IMPLEMENTATION AND SIMULATION IN JULIA {.titleHead}
---------------------------------------------------------

Tyler M. Masthay and Saverio Perugini\
 Department of Computer Science\
 University of Dayton\
 300 College Park\
 Dayton, OhioÂ Â 45469â€“2160\
 (937) 229â€“4079\
 {tmasthay1,saverio}@udayton.edu

\

ABSTRACT

We present a full implementation of the parareal algorithmâ€”an
integration technique to solve diï¬€erential equations in parallelâ€”in the
Julia programming language for a fully general, ï¬rst-order,
initial-value problem. We provide a brief overview of Juliaâ€”a concurrent
programming language for scientiï¬c computing. Our implementation of the
parareal algorithm accepts both coarse and ï¬ne integrators as functional
arguments. We use Eulerâ€™s method and another Runge-Kutta integration
technique as the integrators in our experiments. We also present a
simulation of the algorithm for purposes of pedagogy and as a tool for
investigating the performance of the algorithm.

### INTRODUCTION {.likesectionHead}

The parareal algorithm was ï¬rst proposed in 2001 by Lions, Maday, and
TuriniciÂ [[7](#XLionsEtAl)] as an integration technique to solve
diï¬€erential equations in parallel. We present a full implementation of
the parareal algorithm in the Julia programming language
(<https://julialang.org>)Â [[8](#XJulia)] for a fully general,
ï¬rst-order, initial-value problem. Furthermore, we present a simulation
of the algorithm for purposes of pedagogy and as a tool for
investigating the performance of the algorithm. Our implementation
accepts both coarse and ï¬ne integrators as functional arguments. We use
Eulerâ€™s method and another Runge-Kutta integration technique as the
integrators in our experiments. We start with a brief introduction to
the Julia proramming language.

### AN INTRODUCTION TO JULIA: DYNAMIC, YET EFFICIENT SCIENTIFIC/NUMERICAL PROGRAMMING {.likesectionHead}

Julia is a multi-paradigm language designed for scientiï¬c computing
which supports multidimensional arrays, concurrency, and
metaprogramming. Due to both Juliaâ€™s LLVM-based Just-In-Time (JIT)
compiler and the language design, Julia programs run computationally
eï¬ƒcientâ€”approaching and sometimes matching the speed of languages like
C. SeeÂ [[1](#Xwebsite)] for a graph depicting the relative performance
of Julia compared to other common languages for scientiï¬c computing on a
set of micro-benchmarks.

#### Coroutines and CSP in Julia {.likesubsectionHead}

Coroutines are typically referred to as tasks in Julia, and are not
scheduled to run on separate CPU cores. Channels in Julia can be either
synchronous or asynchronous, and can be typed. However, if no type is
speciï¬ed in the deï¬nition of the channel, then values of any types can
be written to that channel, much like unix pipes.

Messages are passed between coroutines through channels with the put!
and take!() functions. To add tasks to be automatically scheduled, use
the schedule() function, or the @schedule and @sync macros. Of course,
coroutines have little overhead, but will always run on the same cpu.

The current version of Julia multiplexes all tasks onto a single os
thread. Thus, while tasks involving i/o operations beneï¬t from parallel
execution, compute bound tasks are eï¬€ectively executed sequentially on a
single os thread. Future versions of Julia may support scheduling of
tasks on multiple threads, in which case compute bound tasks will enjoy
the beneï¬ts of parallel execution as
wellÂ [[2](#XJuliaParallelComputingMan)].

#### Parallel Computing {.likesubsectionHead}

In addition to tasks, Julia supports parallel computingâ€”functions
running on distributed computers, or multiple cpus. New processes are
spawned with addproc(\<n\>), where \<n\> is the number of processes
desired. The function addproc returns the pids of the created processes.
The function workers returns a list of the processes. Alternatively, the
Julia interpreter can be started with the -p \<n\> option, where \<n\>
is the number of processes desired. For instance:

\$Â juliaÂ \
julia\>Â addprocs(3)Â \
3âˆ’elementÂ Array{Int64,1}:Â \
Â 2Â \
Â 3Â \
Â 4Â \
Â \
julia\>Â workers()Â \
3âˆ’elementÂ Array{Int64,1}:Â \
Â 2Â \
Â 3Â \
Â 4Â \
Â \
Ë†DÂ \
\$Â \
\$Â juliaÂ âˆ’pÂ 3Â \
julia\>Â workers()Â \
3âˆ’elementÂ Array{Int64,1}:Â \
Â 2Â \
Â 3Â \
Â 4Â \
Â \
Ë†DÂ \
\$

Note that the process ids start at 2 because the Julia REPL shell is
process 1.

Processes in Julia which are either locally running or remotely
distributed, communicate with each other through message passing.

The function remotecall(\<Function\>,Â \<ProcessID\>,Â \<args â€¦\>)
executes \<Function\> on worker \<ProcessID\> and returns a value of the
Future type, which contains a reference to a location from which the
return value can be retrieved, once \<Function\> has completed its
execution. The Future value can be extracted with the function fetch(),
which blocks until the result is available. Thus, the function
remotecall is used to send a message while the function fetch is used to
receive a message. For instance:

julia\>Â addprocs(2)Â \
julia\>Â futureÂ =Â remotecall(sqrt,Â 2,Â 4)Â \
Future(2,1,3,Nullable{Any}())Â \
julia\>Â fetch(future)Â \
2.0

After the function remotecall is run, the worker process simply waits
for the next call to remotecall.

julia\>Â counter1Â =Â new\_counter(3)Â \
(::\#1)Â (genericÂ functionÂ withÂ 1Â method)Â \
julia\>Â futureÂ =Â remotecall(counter1,Â 2)Â \
Future(2,1,23,Nullable{Any}())Â \
julia\>Â fetch(future)Â \
4

The Julia macro @spawn simpliï¬es this message-passing protocol for the
programmer and obviates the need for explicit use of the low-level
remotecall function. Similarly, the macro @parallel can be used to run
each iteration of a (for) loop in its own process.

julia\>Â futureÂ =Â @spawnÂ sqrt(4)Â \
julia\>Â fetch(future)Â \
2.0Â \
julia\>Â addprocs(2)Â \
2âˆ’elementÂ Array{Int64,1}:Â \
Â 3Â \
Â 4Â \
julia\>Â @everywhereÂ functionÂ fib(n)Â \
Â Â Â Â ifÂ (nÂ \<Â 2)Â \
Â Â Â Â Â Â Â Â returnÂ nÂ \
Â Â Â Â elseÂ \
Â Â Â Â Â Â Â Â returnÂ fib(nâˆ’1)Â +Â fib(nâˆ’2)Â \
Â Â Â Â endÂ \
endÂ \
julia\>Â @everywhereÂ functionÂ fib\_parallel(n)Â \
Â Â Â Â ifÂ (nÂ \<Â 35)Â \
Â Â Â Â Â Â Â Â returnÂ fib(n)Â \
Â Â Â Â elseÂ \
Â Â Â Â Â Â Â Â xÂ =Â @spawnÂ fib\_parallel(nâˆ’1)Â \
Â Â Â Â Â Â Â Â yÂ =Â fib\_parallel(nâˆ’2)Â \
Â Â Â Â Â Â Â Â returnÂ fetch(x)Â +Â yÂ \
Â Â Â Â endÂ \
endÂ \
julia\>Â @timeÂ fib(42)Â \
Â Â 2.271563Â secondsÂ (793Â allocations:Â 40.718Â KB)Â \
267914296Â \
julia\>Â @timeÂ fib\_parallel(42)Â \
Â Â 3.483601Â secondsÂ (344.48Â kÂ allocations:Â \
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 15.344Â MB,Â 0.25%Â gcÂ time)

There are also remote channels which are writable for more control over
synchronizing processes.

It can accept a user-deï¬ned function that performs the computation in
parallelÂ [[5](#XPPPP:Julia)].

#### Multidimensional Arrays {.likesubsectionHead}

Julia supports multidimensional arrays, which are an important data
structure in scientiï¬c computing applications, with a simple syntax and
their eï¬ƒcient creation and interpretation over many
dimensionsÂ [[4](#XOperators:Julia)]. The function call
ArrayType(\<dimensions\>) creates an array, where the nth argument in
dimensions speciï¬es the size of the nth dimension of the array.
Similarly, the programmer manipulates these arrays using function calls
that support inï¬nite-dimensional arrays given only limitations on
computational time. In summary, Julia incorporates concepts and
mechanismsâ€”particularly concurrency and multidimensional arraysâ€”which
support eï¬ƒcient scientiï¬c computing.

### THE PARAREAL ALGORITHM {.likesectionHead}

* * * * *

![PIC](error.png)\

FigureÂ 1: Right endpoint error.

* * * * *

The parareal algorithm is designed to perform parallel-in-time
integration for a ï¬rst-order initial-value problem. The algorithm
involves two integration techniques, often known as the â€˜coarseâ€™
integrator and the â€˜ï¬neâ€™ integrator. For the algorithm to be eï¬€ective,
the coarse integrator must be of substantially lower computational cost
than the ï¬ne integrator. The reason will become apparent later in this
section. Consider the diï¬€erential equation (1) given by

  ------------------------------------ ------------------------------------
  yâ€²(t) = f(t,y(t))t âˆˆ [a,b]
  (1)
  ------------------------------------ ------------------------------------

with its associated initial-value problem (2)

  ------------------------------------ ------------------------------------
  y(tâˆ—) = yâˆ—tâˆ— âˆˆ [a,b].
  (2)
  ------------------------------------ ------------------------------------

For simplicity, let us assume tâˆ— = a, so that the solution only extends
rightward. To obtain an approximate solution to equationÂ (1) satisfying
the initial conditionÂ (2), we partition our domain into [t0 = a,...,tN =
b] with uniform step size Î”. We now precisely deï¬ne an â€˜integratorâ€™ as a
function from (0,âˆ) Ã— â„2 Ã—â„› to â„ where â„› is the set of all Riemann
integrable functions. For example, the integrator I given by

  -------------------------------
  I(Î´,x0,y0,g) = y0 + g(x0,y0)Î´
  -------------------------------

is the integrator corresponding to Eulerâ€™s method with step size Î´. Let
ğ’ and â„± be the coarse and ï¬ne integrators, respectively. Deï¬ne

y0,1 = y(t0) = yâˆ— yn+1,1 = y(tn+1) = ğ’(Î”,tn,yn,1,f).

Since yn+1,1 depends on yn,1, this algorithm is inherently sequential.
Partition [tn,tn+1] into {tn0 = t n,...,tnm,...t nM = t n+1} with
uniform step size Î´ \< Î”. Deï¬ne

zn,10 = y(t n0) = y n,1 zn,1m+1 = y(t nm+1) = â„±(Î´,t nm,z n,1m,f).

This yields an approximate solution {zn,10,...,z n,1M} to (1) over
[tn,tn+1] with initial conditions

y(tn) = yn,1.

Since zn1,1m1 does not depend on zn2,1m2 for n1â‰ n2, we can compute these
approximations in parallel. After the last subproblem is solved, we
simply combine the solutions on each subdomain to obtain a solution over
the whole interval. However, our values {y1,1,...,yn,1} are relatively
inaccurate. The vertical spikes in the orange graph separating the
coarse and ï¬ne predictions in FigureÂ [1](#x1-60011) illustrate this
error. However, znâˆ’1,1M is a better approximation for Ï•(tn) where Ï• is
the exact solution to the diï¬€erential equation. We use this to obtain a
better set of points {yn,2} for the coarse approximation. We do this by
ï¬rst deï¬ning wn,1 = yn,1 and then deï¬ning

w1,2 = y1,1 = y1,2 = yâˆ— wn,2 = ğ’(Î”,tnâˆ’1,ynâˆ’1,2,f) yn,2 = (wn,2 âˆ’ wn,1) +
znâˆ’1,1M.

Thus, wn+1,2 serves as a new prediction given a more accurate previous
prediction from yn,2 since znâˆ’1,1M has now been taken into account in
calculating yn,2. In general, we continue evaluating so that for k \> 1,
we have

w1,k = y1,k = yâˆ— wn,k = ğ’(Î”,tnâˆ’1,ynâˆ’1,kâˆ’1,f) yn,k = (wn,k âˆ’ wn,kâˆ’1) +
znâˆ’1,kâˆ’1M.

Note that since yn,k is dependent on wn,k, this step must be done
sequentially. As k increases, wn,k âˆ’ wn,kâˆ’1 â†’ 0, which means that yn,k
converges to the value that the ï¬ne integrator would predict if ï¬ne
integration were simply done sequentially. Thus, each k denotes ï¬ne
integration over the whole interval. This means that the total
computation performed is much greater than if ï¬ne integration were
performed sequentially. However, the time eï¬ƒciency of each iteration has
the potential to be improved through concurrency. Since ï¬ne integration
is more computationally intensive, this improvement in the run-time
eï¬ƒciency may compensate for the cumulative computation performed.

Let K be the total number of iterations necessary to achieve a desired
accuracy of solution and P be the number of subintervals into which we
divide according to the coarse integrator. If K = 1, then we achieve
perfect parallel eï¬ƒciency. If K = P, then we likely slowed the
computation down. The parareal algorithm is guaranteed to converge to
the solution given by the sequential ï¬ne integrator within P iterations.
For a more complete treatment of this convergence analysis, we refer the
reader toÂ [[6](#XGander)]. For fully general pseudocode, we refer the
reader toÂ [[3](#XAubanel),Â [9](#XNielsen)].

### PARAREAL ALGORITHM IMPLEMENTATION IN JULIA {.likesectionHead}

* * * * *

ListingÂ 1: Implementation of the parareal algorithm in Julia.

@everywhereÂ functionÂ parareal(a,b,nC,nF,K,y0,f,coarseIntegrator,fineIntegrator)Â \
\#initializeÂ coarseÂ informationÂ \
xCÂ =Â linspace(a,b,nC+1);Â \
yCÂ =Â zeros(size(xC,1),K);Â \
deltaCÂ =Â (bâˆ’a)Â /Â (nCÂ +Â 1);Â \
yC[1,:]Â =Â y0;Â \
Â \
\#â€coarseÂ integratorÂ partiallyÂ evaluatedâ€Â \
ciPEvaledÂ =Â ((x1,y1)Â âˆ’\>Â coarseIntegrator(deltaC,x1,y1,f));Â \
Â \
\#getÂ initialÂ coarseÂ integrationÂ solutionÂ \
forÂ i=2:(nC+1)Â \
Â Â Â yC[i,1]Â =Â ciPEvaled(xC[iâˆ’1],yC[iâˆ’1,1]);Â \
endÂ \
correctCÂ =Â copy(yC);Â \
Â \
\#initializeÂ fineÂ informationÂ \
xFÂ =Â zeros(nC,nF+1);Â \
forÂ i=1:nCÂ \
Â Â Â xF[i,:]Â =Â linspace(xC[i],xC[i+1],nF+1);Â \
endÂ \
subÂ =Â zeros(nC,nF+1,K);Â \
deltaFÂ =Â xF[1,2]Â âˆ’Â xF[1,1];Â \
Â \
\#â€fineÂ integratorÂ partiallyÂ evaluatedâ€Â \
fiPEvaledÂ =Â ((x1,y1)Â âˆ’\>Â fineIntegrator(deltaF,x1,y1,f));Â \
Â \
forÂ k=2:KÂ \
Â Â Â \#runÂ fineÂ integrationÂ onÂ eachÂ subdomainÂ \
Â Â Â tic();Â \
Â Â Â @syncÂ forÂ i=1:nCÂ \
Â Â Â Â Â Â sub[i,1,k]Â =Â correctC[i,kâˆ’1];Â \
Â Â Â Â Â Â @asyncÂ forÂ j=2:(nF+1)Â \
Â Â Â Â Â Â Â Â Â sub[i,j,k]Â =Â fiPEvaled(xF[i,jâˆ’1],sub[i,jâˆ’1,k]);Â \
Â Â Â Â Â Â endÂ \
Â Â Â endÂ \
Â Â Â toc();Â \
Â \
Â Â Â \#predictÂ andÂ correctÂ \
Â Â Â forÂ i=1:nCÂ \
Â Â Â Â Â Â yC[i+1,k]Â =Â ciPEvaled(xC[i],correctC[i,k]);Â \
Â Â Â Â Â Â correctC[i+1,k]Â =Â yC[i+1,k]Â âˆ’Â yC[i+1,kâˆ’1]Â +Â sub[i,nF+1,k];Â \
Â Â Â endÂ \
endÂ \
Â \
yFÂ =Â zeros(nCâˆ—(nF+1),Kâˆ’1);Â \
forÂ k=2:KÂ \
Â Â Â yF[:,kâˆ’1]Â =Â reshape(sub[:,:,k]â€™,nCâˆ—(nF+1));Â \
endÂ \
Â \
returnÂ reshape(xFâ€™,nCâˆ—(nF+1)),reshape(sub[:,:,K]â€™,nCâˆ—(nF+1)),yF,sub,xC,correctC,yC;Â \
end

* * * * *

ListingÂ [1](#x1-7001r1) presents an implementation of the parareal
algorithm (from the prior section) in Julia. The @async macro within the
loop causes the program to evaluate the ï¬rst expression to its right as
a concurrent task (i.e., the for loop assigning values to sub). The
@sync macro causes the main program thread to wait until all tasks
(spawned in the the ï¬rst expression to its right with an @async or
@parallel macro) complete. Once all concurrent tasks are complete,
execution of the program proceeds sequentially. Given the semantics of
these macros, the program in ListingÂ [1](#x1-7001r1) correctly perform
concurrent integration. The sequential and parallel versions of this
implementation have no signiï¬cant diï¬€erences in run-time eï¬ƒciency.
However, if a sleep statement is placed in the argument of
fineIntegrator, the parallel version runs much faster. This demonstrates
that use of those two macros does lead to concurrent program execution.

### GRAPHICAL ALGORITHM SIMULATION {.likesectionHead}

* * * * *

ListingÂ 2: Implementation of a graphical simulator of the parareal
algorithm in Julia.

@everywhereÂ functionÂ fullMethod(n,a,b,y0,f,integrator)Â \
Â Â Â \#setupÂ domainÂ andÂ rangeÂ spaceÂ \
Â Â Â Â xÂ =Â linspace(a,b,n+1);Â \
Â Â Â deltaXÂ =Â x[2]Â âˆ’Â x[1];Â \
Â Â Â Â yÂ =Â ones(n+1,1);Â \
Â \
Â Â Â \#initializeÂ leftÂ endpointÂ \
Â Â Â Â y[1]Â =Â y0;Â \
Â \
Â Â Â \#integrateÂ eachÂ pointÂ \
Â Â Â Â forÂ i=1:nÂ \
Â Â Â Â Â Â Â Â y[i+1]Â =Â integrator(deltaX,x[i],y[i],f);Â \
Â Â Â Â endÂ \
Â Â Â returnÂ x,y;Â \
endÂ \
Â \
functionÂ simulate(a,b,N,M,K,y0,f,coarseInt,fineInt,showPrev)Â \
Â Â Â x1,y1Â =Â fullMethod(Nâˆ—(M+1),a,b,y0,f,fineInt);Â \
Â Â Â x,y,yF,sub,xC,yC,iCÂ =Â parareal(a,b,N,M,K,y0,f,coarseInt,fineInt);Â \
Â Â Â xFÂ =Â (reshape(x,M+1,N))â€™;Â \
Â Â Â fineÂ =Â M+1;Â \
Â Â Â forÂ k=2:KÂ \
Â Â Â Â Â Â display(plot(x1,y1));Â \
Â Â Â Â Â Â if(showPrevÂ &&Â kÂ \>Â 2Â )Â \
Â Â Â Â Â Â Â Â Â display(scatter!(xC,yC[:,kâˆ’2],color=â€redâ€,legend=false));Â \
Â Â Â Â Â Â endÂ \
Â Â Â Â Â Â display(scatter!(xC,yC[:,kâˆ’1],color=â€greenâ€,legend=false));Â \
Â Â Â Â Â Â doneÂ =Â zeros(Int64,N,1);Â \
Â Â Â Â Â Â workingSubdomainsÂ =Â 1:N;Â \
Â Â Â Â Â Â while(doneÂ !=Â (M+1)Â âˆ—Â ones(N,1)Â )Â \
Â Â Â Â Â Â Â Â Â indexÂ =Â Int64(ceil(size(workingSubdomains,1)âˆ—rand()));Â \
Â Â Â Â Â Â Â Â Â currThreadÂ =Â workingSubdomains[index];Â \
Â Â Â Â Â Â Â Â Â while(Â done[currThread]Â ==Â M+1Â )Â \
Â Â Â Â Â Â Â Â Â Â Â Â currThreadÂ =Â Int64(ceil(NÂ âˆ—Â rand()));Â \
Â Â Â Â Â Â Â Â Â endÂ \
Â Â Â Â Â Â Â Â Â currThreadPlotÂ =Â Int64(ceil(fineâˆ—rand()));Â \
Â Â Â Â Â Â Â Â Â totalAdvanceÂ =Â done[currThread]Â +Â currThreadPlot;Â \
Â Â Â Â Â Â Â Â Â if(totalAdvanceÂ \>Â fine)Â totalAdvanceÂ =Â fine;Â endÂ \
Â Â Â Â Â Â Â Â Â newPÂ =Â (done[currThread]+1):totalAdvance;Â \
Â Â Â Â Â Â Â Â Â display(plot!(xF[currThread,newP],sub[currThread,newP,k],color=â€blackâ€));Â \
Â Â Â Â Â Â Â Â Â done[currThread]Â =Â totalAdvance;Â \
Â Â Â Â Â Â Â Â Â workingSubdomainsÂ =Â find(Â ((x)âˆ’\>xÂ !=Â M+1),Â doneÂ );Â \
Â Â Â Â Â Â Â Â Â print(join([â€WorkingÂ onÂ subdomainÂ \#â€,Â currThread,Â â€...â€,Â \
Â Â Â Â Â Â Â Â Â Â Â Â â€PendingÂ Subdomains:Â â€,Â workingSubdomainsâ€™,Â â€âˆ–nâ€]));Â \
Â Â Â Â Â Â endÂ \
Â Â Â Â Â Â display(plot!(x,yF[:,kâˆ’1],color=â€orangeâ€));Â \
Â Â Â Â Â Â sleep(5);Â \
Â Â Â endÂ \
endÂ \
Â \
\#Â ImplementationÂ schemes.Â \
functionÂ euler(delta,x0,y0,f)Â \
Â Â Â returnÂ y0Â +Â deltaÂ âˆ—Â f(x0,y0);Â \
endÂ \
Â \
functionÂ rungeKutta(delta,x0,y0,f)Â \
Â Â Â k1Â =Â f(x0,y0);Â \
Â Â Â k2Â =Â f(x0+delta/2,y0Â +Â (delta/2)âˆ—k1);Â \
Â Â Â k3Â =Â f(x0+delta/2,y0Â +Â (delta/2)âˆ—k2);Â \
Â Â Â k4Â =Â f(x0+delta,y0+deltaâˆ—k3);Â \
Â Â Â returnÂ y0Â +Â (delta/6)âˆ—(k1+2âˆ—k2+2âˆ—k3+k4);Â \
end

* * * * *

* * * * *

  ------------------
  ![PIC](sin1.png)
  ![PIC](sin2.png)
  ------------------

\

FigureÂ 2: Slow parareal example. (left) Solution after ï¬rst iteration
with Eulerâ€™s method. (right) Solution after ninth iteration with Eulerâ€™s
method.

* * * * *

The function simulate in ListingÂ [2](#x1-8001r2) creates a graphical
simulator of the parareal algorithm. This function can be used to
introduce the parareal algorithm to students in a numerical analysis
course. The ï¬rst line gets the sequential solution from the ï¬ne
integrator (the â€˜idealâ€™ solution) and the second line gets the history
of the computations that took place during the parareal execution. The
main loop over the variable k then displays the inner workings of the
algorithm. The ideal solution is plotted, with a scatter plot of the
points obtained from the coarse integrator. To simulate the parallel
nature of the algorithm, random progress is made on randomly selected
subdomains. Thus, the plot dynamically makes partial progress on
diï¬€erent subdomains until all subdomains are ï¬nished with the ï¬ne
integration. After this, the plots are connected into the current
iterationâ€™s approximation. During the next iteration, the previous
guesses from the coarse integrator are displayed in red and the new
guesses from the coarse integrator are displayed in green. As k
increases, these guesses converge to the ideal solution.

In addition to the use of this function for pedagogical purposes, it can
be used to investigate the types of curves for which the parareal
algorithm might be practical. For instance, consider the diï¬€erential
equation

  -------------------------------
  yâ€²(x) = sin(xy),x âˆˆ [âˆ’20, 20]
  -------------------------------

with y(âˆ’20) = 10, Î” = 4 (10 points), and Î´ = 0.008 (500 points).
FigureÂ [2](#x1-80652) shows the ï¬rst and ninth iterations. The ninth
iterationâ€™s large error on the right end of the interval shows that this
is an example where parareal convergence is slow. This is as ineï¬ƒcient
as possible, needing as many iterations as subdomains in order for the
solution to converge. However, the simulation also shows that if f(x,y)
= sin(x)ex, then the solution converges after just one iteration. These
two examples show that the algorithmâ€™s eï¬ƒciency can be highly dependent
on the integrand. Below the simulation function are Eulerâ€™s method and
another Runge-Kutta integration technique that can be used as examples
to be passed as ï¬rst-class functions as coarse or ï¬ne integration
techniques to the parareal or simulate functions. A Git repository of
both the implementation and graphical simulation is available in GitHub
at
<https://github.com/sperugin/Parareal-Implementation-and-Simulation-in-Julia.git>.
All of the graphical plots are generated with the Julia Plots package
available at <https://juliaplots.github.io/>. A video describing this
application of Julia is available on YouTube at
<https://www.youtube.com/watch?v=MtgbeLO6ZM4>.

### REFERENCES {.likesectionHead}

[1]Â Â Â High performance JIT compiler. Available: <http://julialang.org>
[Last accessed: 24 April 2017].

[2]Â Â Â Julia documentation: Parallel computing. Available:
[https://docs.julialang.org/en/stable/manual/parallel-computing/\\\#Parallel-Computing-1](https://docs.julialang.org/en/stable/manual/parallel-computing/\#Parallel-Computing-1)
[Last accessed: 22 May 2018].

[3]Â Â Â Aubanel, E. Scheduling of tasks in the parareal algorithm.
Parallel Computing, 37(3):172â€“182, 2011.

[4]Â Â Â Bezanson, J., Chen, J., Karpinski, S., Shah, V., and Edelman, A.
Array operators using multiple dispatch: A design methodology for array
implementations in dynamic languages. In Proceedings of ACM SIGPLAN
International Workshop on Libraries, Languages, and Compilers for Array
Programming, pages 56â€“61, New York, NY, 2014. ACM Press.

[5]Â Â Â Chen, J. and Edelman, A. Parallel preï¬x polymorphism permits
parallelization, presentation and proof. In Proceedings of the First
Workshop for High Performance Technical Computing in Dynamic Languages
(HPTCDL), pages 47â€“56, Piscataway, NJ, USA, 2014. IEEE Press.

[6]Â Â Â Gander, M. and Vandewalle, S. Analysis of the parareal
time-parallel time-integration method. SIAM Journal on Scientiï¬c
Computing, 29(2):556â€“578, 2007.

[7]Â Â Â Lions, J.-L., Maday, Y., and Turinici, G. A â€œpararealâ€ in time
discretization of pdeâ€™s. Comptes Rendus de lâ€™AcadÃ©mie des Sciences -
Series I - Mathematics, 332:661â€“668, 2001.

[8]Â Â Â Moï¬ƒt, J. and Tate, B. Julia. In Tate, B., Daoud, F., Dees, I., and
Moï¬ƒt, J., editors, Seven more languages in seven weeks: Languages that
are shaping the future, chapterÂ 5, pages 171â€“207. Pragmatic Bookshelf,
Dallas, TX, 2014.

[9]Â Â Â Nielsen, A. Feasibility study of the parareal algorithm. Masterâ€™s
thesis, Technical University of Denmark, 2012.

